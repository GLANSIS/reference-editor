{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c18ecca1",
   "metadata": {},
   "source": [
    "# GLANSIS REFERENCE CLEANER & BULK UPLOADER\n",
    "**Description:** The following scripts will help you to clean and format references for bulk uploader.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acc860f",
   "metadata": {},
   "source": [
    "# PART 1: REFERENCE CLEANER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24d1dad",
   "metadata": {},
   "source": [
    "## 1. Exporting from EndNote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bf9c18",
   "metadata": {},
   "source": [
    "To correctly get your Journal Article references out of Endnote into a text file that Jupyter Notebooks can read, see the following steps:\n",
    "1. Download and save the GLANSIS_refManagerExport.\n",
    "2. Select and open output style. It will open an EndNote output style editor.\n",
    "3. Click File > Save As.\n",
    "4. Close output style editor.\n",
    "5. Go to hte task bar and select the 'Biolographic Output Style' dropdown bar. Click 'Select Another Style.' Scroll to and select 'GLANSIS_refManagerExport.' Click 'Choose.'\n",
    "6. Select all the journal articles you want to enter into NAS.\n",
    "7. Click File > Export. In the pop-up menu, make to change preferences to:<br>\n",
    "    a. File name: Enter your file name of choice<br>\n",
    "    b. Save as type: Text File (.txt)<br>\n",
    "    c. Output style: GLANSIS_refManagerExport<br>\n",
    "    d. Click Save<br>\n",
    "8. To make it easier for the code to find a PDF, extract all PDFs from EndNote library and put into a new folder.<br>\n",
    "    a. Go to directory where the EndNote Library is stored. Click on the .Data folder.<br>\n",
    "    b. There will be a PDF and sdb folder inside. Click on the PDF folder.<br>\n",
    "    c. In the search bar type '.pdf'. Click on the first PDF and hit Ctrl + A.<br>\n",
    "    d. Copy and paste PDFs into a new folder. I recommend in the same folder as your EndNote Library outside your .Data file<br>\n",
    "9. Once this is done, you should be ready to start editing your\n",
    "    \n",
    "\n",
    "*This will only work for journal articles which is the bulk of what we handle. Any reports, websites, or other references will need to be entered by hand.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd117aaf",
   "metadata": {},
   "source": [
    "## 2. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0da4ffd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\redinger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT\n",
    "\n",
    "import os                            # Mananages directories\n",
    "import numpy as np                   # Used to manage NA values\n",
    "import pandas as pd                  # Manages dataframes\n",
    "import re                            # Edits text strings\n",
    "from tkinter import Tk, filedialog   # Creates file dialog box\n",
    "import requests                      # Pulls HTML code from webpage\n",
    "from bs4 import BeautifulSoup        # HTML parsing\n",
    "import fitz                          # Open and pdf manipulation - package for PyMuPDF\n",
    "from docx import Document            # Create and edit Word Document\n",
    "from collections import OrderedDict  # Use to remove duplicates from keywords\n",
    "import unicodedata                   # Convert extracted keywords to unicode - removes issues with duplication\n",
    "import nltk                          # Library of natural language processing tools              \n",
    "import nltk.corpus                   # Access corpora\n",
    "import string                        # Format text strings\n",
    "import pickle                        # Serailizes and deserializing models\n",
    "import math                          # Math functions for numerical computations\n",
    "import nltk.data                     # Retrieves data files and resources\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer   # Concatenates token sequences\n",
    "from nltk.tokenize import word_tokenize   # Splits words into tokens\n",
    "nltk.download('punkt')                    # Pre-trained tokenizer\n",
    "from openpyxl import load_workbook        # Edit Excel sheets\n",
    "from openpyxl.styles import PatternFill   # Modify Excel formatting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fea28e1",
   "metadata": {},
   "source": [
    "## 3. Set Up for Cleaning:\n",
    "Add the relavent scientific and common names for your current species. These will be used for the creation of key words. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a80ff04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include any relevant scientific name and common name\n",
    "species_names = ['Cyprinella whipplei', 'steelcolor shiner']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0a05f2",
   "metadata": {},
   "source": [
    "By running the cell below, you will open a file dialog box with your computers directory. You may have to minimize the current window to see it (sometimes it likes to hide behind other open windows). Then travel to the folder containing the excel sheet with your references that need cleaned. Select the file and hit 'open.' The first five rows of reference sheet will appear below if you have uploaded correctly. Check to make sure everything looks correct.\n",
    "\n",
    "* Note: EndNote sometimes has trouble with formatting references from older PDFs. If you find yourself getting an error code like this: ''. Try importing the .txt file into Excel as a comma delimited file to find the problem reference. Those references unfortunantly will need to be entered by hand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ae9707d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Author</th>\n",
       "      <th>Year</th>\n",
       "      <th>Title</th>\n",
       "      <th>Journal Name</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Issue</th>\n",
       "      <th>Pages</th>\n",
       "      <th>URL</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>DOI</th>\n",
       "      <th>PDF Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Journal Article</td>\n",
       "      <td>Faisal, M., M. Shavalier, R. K. Kim, E. V. Mil...</td>\n",
       "      <td>2012</td>\n",
       "      <td>Spread of the emerging viral hemorrhagic septi...</td>\n",
       "      <td>Viruses</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>734-60</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pubmed/22754647</td>\n",
       "      <td>Animal Structures/pathology/virology\\r\\nAnimal...</td>\n",
       "      <td>In 2003, viral hemorrhagic septicemia virus (V...</td>\n",
       "      <td>10.3390/v4050734</td>\n",
       "      <td>internal-pdf://1906415602/Faisal-2012-Spread o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Journal Article</td>\n",
       "      <td>Girard, C. M. D.</td>\n",
       "      <td>1857</td>\n",
       "      <td>Researches upon the cyprinoid fishes inhabitin...</td>\n",
       "      <td>Proceedings of the Academy of Natural Sciences...</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>165-213</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>internal-pdf://3202618271/Girard-1857-Research...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Journal Article</td>\n",
       "      <td>Levengood, J. M., and D. J. Schaeffer</td>\n",
       "      <td>2011</td>\n",
       "      <td>Polycyclic aromatic hydrocarbons in fish and c...</td>\n",
       "      <td>Ecotoxicology</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>1411-21</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pubmed/21594573</td>\n",
       "      <td>Animals\\r\\nAstacoidea/*metabolism\\r\\nEnvironme...</td>\n",
       "      <td>We identified and quantified polyaromatic hydr...</td>\n",
       "      <td>10.1007/s10646-011-0698-x</td>\n",
       "      <td>internal-pdf://2614282786/Levengood-2011-Polyc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Type                                             Author  Year  \\\n",
       "0  Journal Article  Faisal, M., M. Shavalier, R. K. Kim, E. V. Mil...  2012   \n",
       "1  Journal Article                                   Girard, C. M. D.  1857   \n",
       "2  Journal Article              Levengood, J. M., and D. J. Schaeffer  2011   \n",
       "\n",
       "                                               Title  \\\n",
       "0  Spread of the emerging viral hemorrhagic septi...   \n",
       "1  Researches upon the cyprinoid fishes inhabitin...   \n",
       "2  Polycyclic aromatic hydrocarbons in fish and c...   \n",
       "\n",
       "                                        Journal Name Volume Issue    Pages  \\\n",
       "0                                            Viruses      4     5   734-60   \n",
       "1  Proceedings of the Academy of Natural Sciences...      8   NaN  165-213   \n",
       "2                                      Ecotoxicology     20     6  1411-21   \n",
       "\n",
       "                                            URL  \\\n",
       "0  https://www.ncbi.nlm.nih.gov/pubmed/22754647   \n",
       "1                                           NaN   \n",
       "2  https://www.ncbi.nlm.nih.gov/pubmed/21594573   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  Animal Structures/pathology/virology\\r\\nAnimal...   \n",
       "1                                                NaN   \n",
       "2  Animals\\r\\nAstacoidea/*metabolism\\r\\nEnvironme...   \n",
       "\n",
       "                                            Abstract  \\\n",
       "0  In 2003, viral hemorrhagic septicemia virus (V...   \n",
       "1                                                NaN   \n",
       "2  We identified and quantified polyaromatic hydr...   \n",
       "\n",
       "                         DOI  \\\n",
       "0           10.3390/v4050734   \n",
       "1                        NaN   \n",
       "2  10.1007/s10646-011-0698-x   \n",
       "\n",
       "                                            PDF Name  \n",
       "0  internal-pdf://1906415602/Faisal-2012-Spread o...  \n",
       "1  internal-pdf://3202618271/Girard-1857-Research...  \n",
       "2  internal-pdf://2614282786/Levengood-2011-Polyc...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open a file dialog to select an Excel file\n",
    "root = Tk()                                # Initialize\n",
    "root.attributes(\"-topmost\", True)          # Ensure dialog is on top of other windows\n",
    "root.withdraw()                            # Hide the root window\n",
    "file_path = filedialog.askopenfilename()   # Open dialogue box\n",
    "\n",
    "# Create new column names because text file has no header\n",
    "col_names = [\"Type\", \"Author\", \"Year\", \"Title\", \"Journal Name\", \"Volume\", \"Issue\", \"Pages\", \"URL\", \"Keywords\", \"Abstract\", \"DOI\", \"PDF Name\"]\n",
    "\n",
    "# Convert text file into a dataframe\n",
    "df = pd.read_csv(file_path, sep = '\\t', header = None, dtype = str, names = col_names, quotechar = '\"')\n",
    "\n",
    "# Top five rows\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed90e4f7",
   "metadata": {},
   "source": [
    "Select the file location of the pdfs you want uploaded:\n",
    "Before running the cell below, make sure that your PDFs files are in seperate folder. After running this cell, another file dialog box will open. Select the folder where you have stored the PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40d9589f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT EDIT\n",
    "\n",
    "# Open a file dialog to select PDF file location\n",
    "root = Tk()                                         \n",
    "root.attributes(\"-topmost\", True)                   \n",
    "root.withdraw()\n",
    "pdf_folder = filedialog.askdirectory()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf35dd6",
   "metadata": {},
   "source": [
    "## 4. Cleaning References:\n",
    "\n",
    "The following code bloack do quite a bit:\n",
    "* Finds Duplicates\n",
    "* Add Location column with 'NAS' as location\n",
    "* Add 'Species Data Entered' with 'N'\n",
    "* Add 'Impact Data Entered' with 'N'\n",
    "* Extracts keywords from PDFs and adds in scientific and commmon names\n",
    "* Removes illegal characters from abstracts\n",
    "* Removes URL if DOI if present\n",
    "* Cleans DOI column\n",
    "* Cleans PDF file names column\n",
    "* Corrects title capitalization\n",
    "* Adds hypertext to species names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd507cb",
   "metadata": {},
   "source": [
    "### FInd Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b613ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT EDIT!\n",
    "\n",
    "# Duplications \n",
    "\n",
    "#url creation\n",
    "url = 'https://nas.er.usgs.gov/queries/references/ReferenceList.aspx'\n",
    "\n",
    "results = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    # extract search parameters references\n",
    "    lead_author = str(row['Author']).split(',', 1)[0] + ','\n",
    "    date = row['Year']\n",
    "    title_start = ' '.join(str(row['Title']).split()[:5])\n",
    "    \n",
    "    #set queary parameters\n",
    "    search_params = {\n",
    "        \"refnum\": '',\n",
    "        \"author\": lead_author,\n",
    "        \"date\": date,\n",
    "        \"title\":title_start,\n",
    "        \"journal\": '',\n",
    "        \"publisher\": '',\n",
    "        \"vol\": '',\n",
    "        \"issue\":'',\n",
    "        \"pages\":'',\n",
    "        \"URL\":'',\n",
    "        \"key_words\":'',\n",
    "        \"type\":'' \n",
    "    }\n",
    "    \n",
    "    # call url\n",
    "    response = requests.post(url, params = search_params)\n",
    "     \n",
    "    # scrape the RefNum \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        #find specific tag using id attribute\n",
    "        desired_a_tag = soup.find(\"a\", {\"id\": \"ContentPlaceHolder1_GridView1_HyperLink1_0\"})\n",
    "        \n",
    "        if desired_a_tag:\n",
    "            refnum = desired_a_tag.get_text(strip = True)\n",
    "            results.append(refnum)\n",
    "        else:\n",
    "            refnum = 'No'\n",
    "            results.append(refnum)\n",
    "            \n",
    " # add column\n",
    "df['Duplicate'] = results \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8a245e",
   "metadata": {},
   "source": [
    "### Add Location, Specimen Data Entered, Impacts Data Entered, and Comments column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "567b3358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT EDIT\n",
    "\n",
    "# Add for location, specimen data, and impact data\n",
    "df['Location'] = 'NAS'\n",
    "df['Specimen Data Entered'] = 'N'\n",
    "df['Impacts Data Entered'] = 'N'\n",
    "df['Comments'] = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f349d3d8",
   "metadata": {},
   "source": [
    "### Clean Abstract, URL, DOI, and PDF Name columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7a4d95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT EDIT\n",
    "\n",
    "# Clean DOI\n",
    "def clean_doi(text):\n",
    "    if pd.notna(text):\n",
    "        if 'doi.org' in text:\n",
    "            return text.replace('https://doi.org/', '')\n",
    "        else:\n",
    "            return text\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "df['DOI'] = df['DOI'].apply(clean_doi)\n",
    "\n",
    "\n",
    "# Define a function to remove illegal characters\n",
    "def remove_illegal_chars(text):\n",
    "    if pd.isnull(text):  # Check if the cell is empty\n",
    "        return ''\n",
    "    # Define the pattern for illegal characters\n",
    "    illegal_chars_pattern = re.compile(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F-\\x9F]')\n",
    "    \n",
    "    # Replace illegal characters with an empty string\n",
    "    cleaned_text = illegal_chars_pattern.sub('', text)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "df['Abstract'] = df['Abstract'].apply(remove_illegal_chars) \n",
    "    \n",
    "# URL Cleaning\n",
    "def clear_cell(row):\n",
    "    if not pd.isna(row['DOI']):\n",
    "        return np.nan\n",
    "    else:\n",
    "        return row['URL']\n",
    "    \n",
    "df['URL'] = df.apply(clear_cell, axis = 1)\n",
    "\n",
    "# Remove bad URLs\n",
    "def remove_non_url(text):\n",
    "    if pd.notna(text):\n",
    "        if '<Go to ISI>:' in text:\n",
    "            return None\n",
    "        else:\n",
    "            return text\n",
    "\n",
    "df['URL'] = df['URL'].apply(remove_non_url)\n",
    "\n",
    "# Clean PDF file name\n",
    "def clean_pdf_name(text):\n",
    "    if isinstance(text, str):\n",
    "        pdf_name = re.search(r'[^/]+$', text).group()\n",
    "        return pdf_name\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "\n",
    "df['PDF Name'] = df['PDF Name'].apply(clean_pdf_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6ab0cc",
   "metadata": {},
   "source": [
    "### Add Keywords:\n",
    "Run one time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3947ec45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT EDIT\n",
    "# RUN ONE TIME\n",
    "\n",
    "# Create new word document for errors\n",
    "doc = Document()\n",
    "style = doc.styles['Normal']\n",
    "style.paragraph_format.space_after = 1\n",
    "\n",
    "# Add keywords\n",
    "def keyword_find(filename):\n",
    "    \n",
    "    # Open file\n",
    "    file = fitz.open(filename)\n",
    "    \n",
    "    # Read and block only the first two pages of PDF\n",
    "    text = []\n",
    "    for i, page in enumerate(file):\n",
    "        if i > 1:\n",
    "            break\n",
    "        text += page.get_text(\"blocks\")\n",
    "    \n",
    "    # Close file\n",
    "    file.close()\n",
    "    \n",
    "    # Create if loop to account PDFs that can't be read in\n",
    "    if text:\n",
    "    \n",
    "        # Find block containing keywords - if no keywords in PDF, common & scientific names used\n",
    "        for block in text:\n",
    "            if block[4].lower().startswith('key-words:'):\n",
    "                pdf_keywords = block[4][10:].strip()\n",
    "                break\n",
    "            elif block[4].lower().startswith('key words:'):\n",
    "                pdf_keywords = block[4][10:].strip()\n",
    "                break\n",
    "            elif block[4].lower().startswith('keywords:'):\n",
    "                pdf_keywords = block[4][9:].strip()\n",
    "                break\n",
    "            elif block[4].lower().startswith('key-words'):\n",
    "                pdf_keywords = block[4][9:].strip()\n",
    "                break\n",
    "            elif block[4].lower().startswith('key words'):\n",
    "                pdf_keywords = block[4][9:].strip()\n",
    "                break\n",
    "            elif block[4].lower().startswith('keywords'):\n",
    "                pdf_keywords = block[4][8:].strip()\n",
    "                break\n",
    "            else:\n",
    "                pdf_keywords = '' \n",
    "                \n",
    "    else:\n",
    "        pdf_keywords = ''\n",
    "        \n",
    "    # Replace intermediate characters - this list is not exhaustive\n",
    "    keywords_replace = re.sub(r'[\\n;�\\xa0·./]', ', ', pdf_keywords).replace(', ,', ',').strip(',')\n",
    "    \n",
    "    # Combine keywords with scientific and common names\n",
    "    clean_keywords = keywords_replace.split(',')\n",
    "    \n",
    "    return(clean_keywords)\n",
    "\n",
    "\n",
    "# Remove breaks created by EndNote\n",
    "df['Keywords'] = df['Keywords'].str.replace('\\r\\n', ', ')\n",
    "\n",
    "# Convert each row into a list\n",
    "df['Keywords'] = df['Keywords'].apply(lambda x: [x] if pd.notnull(x) else [])\n",
    "\n",
    "# Iterate through DataFrame rows\n",
    "for index, row in df.iterrows():\n",
    "    try:\n",
    "        # Create file path\n",
    "        file_path = os.path.join(pdf_folder, row['PDF Name'])\n",
    "        \n",
    "        # Combine keywords entered by user, from EndNote, and from PDF - need to be lists not strings\n",
    "        combined_keywords = species_names + row['Keywords'] + keyword_find(file_path)\n",
    "        \n",
    "        # Remove duplicate keywords\n",
    "        unique_keywords = list(OrderedDict.fromkeys(combined_keywords))\n",
    "        \n",
    "        # Remove extra spaces around words in list\n",
    "        unique_keywords_strip = [word.strip() for word in unique_keywords] \n",
    "        \n",
    "        # Combine list into text string\n",
    "        keywords = ', '.join(unique_keywords_strip)\n",
    "        \n",
    "        # Update 'Keyword' column\n",
    "        df.at[index, 'Keywords'] = keywords\n",
    "        \n",
    "        \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error row {index + 1}: {e} \\nAuthors: {row['Author']} \\nYear: {row['Year']} \\nTitle: {row['Title']} \\nPDF: {row['PDF Name']}\\n\")\n",
    "        \n",
    "        # Write error message to the Word document\n",
    "        doc.add_paragraph(f\"Error row {index + 1}: {e}\")\n",
    "        doc.add_paragraph(f\"Authors: {row['Author']}\")\n",
    "        doc.add_paragraph(f\"Year: {row['Year']}\")\n",
    "        doc.add_paragraph(f\"Title: {row['Title']}\")\n",
    "        doc.add_paragraph(\"\")  # Add an empty line between errors\n",
    "        \n",
    "        # Skip to the next iteration of the loop\n",
    "        continue \n",
    "        \n",
    " # Remove illegal characters\n",
    "df['Keywords'] = df['Keywords'].apply(remove_illegal_chars)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867efcc2",
   "metadata": {},
   "source": [
    "### Title Correction:\n",
    "The following code uses a machine learning model to 'truecase' the journal article titles. \n",
    "\n",
    "* Be Aware!: The underlying model for this is not perfect/needs more work. There will be some errors because there is not yet a good way to account for all variation in proper nouns. If you would rather do this step yourself, you don't need to run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0b1e062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT EDIT\n",
    "\n",
    "class TrueCaser(object):\n",
    "    def __init__(self, dist_file_path=None):\n",
    "\n",
    "        \"\"\" Initialize module with the model from Google Drive \"\"\"\n",
    "        if dist_file_path is None:\n",
    "            dist_file_path = 'models/truecaserTest.dist'\n",
    "            \n",
    "        with open(dist_file_path, \"rb\") as distributions_file:\n",
    "            pickle_dict = pickle.load(distributions_file)\n",
    "            self.uni_dist = pickle_dict[\"uni_dist\"]\n",
    "            self.backward_bi_dist = pickle_dict[\"backward_bi_dist\"]\n",
    "            self.forward_bi_dist = pickle_dict[\"forward_bi_dist\"]\n",
    "            self.trigram_dist = pickle_dict[\"trigram_dist\"]\n",
    "            self.word_casing_lookup = pickle_dict[\"word_casing_lookup\"]\n",
    "        self.detknzr = TreebankWordDetokenizer()\n",
    "\n",
    "    def get_score(self, prev_token, possible_token, next_token):\n",
    "        pseudo_count = 5.0\n",
    "\n",
    "        # Get Unigram Score\n",
    "        numerator = self.uni_dist[possible_token] + pseudo_count\n",
    "        denominator = 0\n",
    "        for alternativeToken in self.word_casing_lookup[\n",
    "                possible_token.lower()]:\n",
    "            denominator += self.uni_dist[alternativeToken] + pseudo_count\n",
    "\n",
    "        unigram_score = numerator / denominator\n",
    "\n",
    "        # Get Backward Score\n",
    "        bigram_backward_score = 1\n",
    "        if prev_token is not None:\n",
    "            numerator = (\n",
    "                self.backward_bi_dist[prev_token + \"_\" + possible_token] +\n",
    "                pseudo_count)\n",
    "            denominator = 0\n",
    "            for alternativeToken in self.word_casing_lookup[\n",
    "                    possible_token.lower()]:\n",
    "                denominator += (self.backward_bi_dist[prev_token + \"_\" +\n",
    "                                                      alternativeToken] +\n",
    "                                pseudo_count)\n",
    "\n",
    "            bigram_backward_score = numerator / denominator\n",
    "\n",
    "        # Get Forward Score\n",
    "        bigram_forward_score = 1\n",
    "        if next_token is not None:\n",
    "            next_token = next_token.lower()  # Ensure it is lower case\n",
    "            numerator = (\n",
    "                self.forward_bi_dist[possible_token + \"_\" + next_token] +\n",
    "                pseudo_count)\n",
    "            denominator = 0\n",
    "            for alternativeToken in self.word_casing_lookup[\n",
    "                    possible_token.lower()]:\n",
    "                denominator += (\n",
    "                    self.forward_bi_dist[alternativeToken + \"_\" + next_token] +\n",
    "                    pseudo_count)\n",
    "\n",
    "            bigram_forward_score = numerator / denominator\n",
    "\n",
    "        # Get Trigram Score\n",
    "        trigram_score = 1\n",
    "        if prev_token is not None and next_token is not None:\n",
    "            next_token = next_token.lower()  # Ensure it is lower case\n",
    "            numerator = (self.trigram_dist[prev_token + \"_\" + possible_token +\n",
    "                                           \"_\" + next_token] + pseudo_count)\n",
    "            denominator = 0\n",
    "            for alternativeToken in self.word_casing_lookup[\n",
    "                    possible_token.lower()]:\n",
    "                denominator += (\n",
    "                    self.trigram_dist[prev_token + \"_\" + alternativeToken +\n",
    "                                      \"_\" + next_token] + pseudo_count)\n",
    "\n",
    "            trigram_score = numerator / denominator\n",
    "\n",
    "        result = (math.log(unigram_score) + math.log(bigram_backward_score) +\n",
    "                  math.log(bigram_forward_score) + math.log(trigram_score))\n",
    "\n",
    "        return result\n",
    "\n",
    "    def first_token_case(self, raw):\n",
    "        return raw.capitalize()\n",
    "\n",
    "    def get_true_case(self, sentence, out_of_vocabulary_token_option=\"title\"):\n",
    "        \"\"\" Wrapper function for handling untokenized input.\n",
    "\n",
    "        @param sentence: a sentence string to be tokenized\n",
    "        @param outOfVocabularyTokenOption:\n",
    "            title: Returns out of vocabulary (OOV) tokens in 'title' format\n",
    "            lower: Returns OOV tokens in lower case\n",
    "            as-is: Returns OOV tokens as is\n",
    "\n",
    "        Returns (str): detokenized, truecased version of input sentence\n",
    "        \"\"\"\n",
    "        tokens = word_tokenize(sentence)\n",
    "        tokens_true_case = self.get_true_case_from_tokens(tokens, out_of_vocabulary_token_option)\n",
    "        return self.detknzr.detokenize(tokens_true_case)\n",
    "\n",
    "    def get_true_case_from_tokens(self, tokens, out_of_vocabulary_token_option=\"title\"):\n",
    "        \"\"\" Returns the true case for the passed tokens.\n",
    "\n",
    "        @param tokens: List of tokens in a single sentence\n",
    "        @param pretokenised: set to true if input is alreay tokenised (e.g. string with whitespace between tokens)\n",
    "        @param outOfVocabularyTokenOption:\n",
    "            title: Returns out of vocabulary (OOV) tokens in 'title' format\n",
    "            lower: Returns OOV tokens in lower case\n",
    "            as-is: Returns OOV tokens as is\n",
    "\n",
    "        Returns (list[str]): truecased version of input list\n",
    "        of tokens\n",
    "        \"\"\"\n",
    "        tokens_true_case = []\n",
    "        for token_idx, token in enumerate(tokens):\n",
    "\n",
    "            if token in string.punctuation or token.isdigit():\n",
    "                tokens_true_case.append(token)\n",
    "            else:\n",
    "                token = token.lower()\n",
    "                if token in self.word_casing_lookup:\n",
    "                    if len(self.word_casing_lookup[token]) == 1:\n",
    "                        tokens_true_case.append(\n",
    "                            list(self.word_casing_lookup[token])[0])\n",
    "                    else:\n",
    "                        prev_token = (tokens_true_case[token_idx - 1]\n",
    "                                      if token_idx > 0 else None)\n",
    "                        next_token = (tokens[token_idx + 1]\n",
    "                                      if token_idx < len(tokens) - 1 else None)\n",
    "\n",
    "                        best_token = None\n",
    "                        highest_score = float(\"-inf\")\n",
    "\n",
    "                        for possible_token in self.word_casing_lookup[token]:\n",
    "                            score = self.get_score(prev_token, possible_token,\n",
    "                                                   next_token)\n",
    "\n",
    "                            if score > highest_score:\n",
    "                                best_token = possible_token\n",
    "                                highest_score = score\n",
    "\n",
    "                        tokens_true_case.append(best_token)\n",
    "\n",
    "                    if token_idx == 0:\n",
    "                        tokens_true_case[0] = self.first_token_case(\n",
    "                            tokens_true_case[0])\n",
    "\n",
    "                else:  # Token out of vocabulary\n",
    "                    if out_of_vocabulary_token_option == \"title\":\n",
    "                        tokens_true_case.append(token.title())\n",
    "                    elif out_of_vocabulary_token_option == \"capitalize\":\n",
    "                        tokens_true_case.append(token.capitalize())\n",
    "                    elif out_of_vocabulary_token_option == \"lower\":\n",
    "                        tokens_true_case.append(token.lower())\n",
    "                    else:\n",
    "                        tokens_true_case.append(token)\n",
    "\n",
    "        return tokens_true_case\n",
    "    \n",
    "# Upload Truecaser model\n",
    "caser = TrueCaser('models/truecaserTest.dist')\n",
    "\n",
    "# Function to apply get_true_case to a sentence\n",
    "def apply_true_case(sentence):\n",
    "    return caser.get_true_case(sentence, \"lower\")\n",
    "\n",
    "\n",
    "# Apply the function to the 'sentences' column\n",
    "df['Title'] = df['Title'].apply(apply_true_case)\n",
    "\n",
    "# Function to capitalize the first letter of the first word\n",
    "def capitalize_first_word(text):\n",
    "    return text[:1].capitalize() + text[1:]\n",
    "\n",
    "# Apply the function to the specified column\n",
    "df['Title'] = df['Title'].apply(capitalize_first_word)\n",
    "\n",
    "\n",
    "# Function to remove <i> and </i> tags from the text\n",
    "def remove_italic_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "# Apply the function to the 'text_column' and store the result in a new column\n",
    "df['Title'] = df['Title'].apply(remove_italic_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0906d86c",
   "metadata": {},
   "source": [
    "### HTML Hypertext\n",
    "\n",
    "Run one time!\n",
    "\n",
    "This will add hypertext (&lt;em&gt;) around scientific names to italicize them in the titles and abstracts.\n",
    "\n",
    "* Adding in hypertext can make it difficult to read the titles. If you want to enter this yourself, you do not need to run the following code. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55d15b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DO NOT EDIT\n",
    "# RUN ONE TIME\n",
    "\n",
    "# Upload text file with species names with subspecies list\n",
    "species_names_with_subspecies = pd.read_excel('textFiles/subspecies.xlsx', header = None)\n",
    "species_names_with_subspecies = species_names_with_subspecies.iloc[:, 0].to_list()\n",
    "\n",
    "# Upload text file with species names list\n",
    "species_names = pd.read_excel('textFiles/scientific_names.xlsx', header = None)\n",
    "species_names = species_names.iloc[:, 0].to_list()\n",
    "\n",
    "# Upload text file with genus list\n",
    "genus = pd.read_excel('textFiles/genus.xlsx', header = None)\n",
    "genus = genus.iloc[:, 0].to_list()\n",
    "\n",
    "# Forumala to surround text with <em>\n",
    "def surround_with_em(text, words_to_surround):\n",
    "    if isinstance(text, str):\n",
    "        for word in words_to_surround:\n",
    "            pattern = r'(?<!<em>)\\b' + re.escape(word.strip()) + r'\\b(?!<\\/em>)'\n",
    "            replacement = r'<em>\\g<0></em>'\n",
    "            text = re.sub(pattern, replacement, text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# Apply the replacement to species names with subspecies\n",
    "df['Title'] = df['Title'].apply(lambda x: surround_with_em(x, species_names_with_subspecies))\n",
    "df['Abstract'] = df['Abstract'].apply(lambda x: surround_with_em(x, species_names_with_subspecies))\n",
    "\n",
    "# Apply the replacement to species names\n",
    "df['Title'] = df['Title'].apply(lambda x: surround_with_em(x, species_names))\n",
    "df['Abstract'] = df['Abstract'].apply(lambda x: surround_with_em(x, species_names))\n",
    "\n",
    "# Apply the replacement to genus\n",
    "df['Title'] = df['Title'].apply(lambda x: surround_with_em(x, genus))\n",
    "df['Abstract'] = df['Abstract'].apply(lambda x: surround_with_em(x, genus))\n",
    "\n",
    "\n",
    "# Formula to abbreviate scientific names\n",
    "def abbreviate_scientific_name(scientific_names):\n",
    "    \n",
    "    abbreviated_names = []\n",
    "        \n",
    "    for name in scientific_names:\n",
    "        words = name.split()\n",
    "        genus = words[0][0] + \".\"\n",
    "        species = \" \".join(words[1:])\n",
    "        abbreviated_names.append(genus + \" \" + species)\n",
    "        \n",
    "    return abbreviated_names\n",
    "\n",
    "abbreviated_names = abbreviate_scientific_name(species_names)\n",
    "\n",
    "# Apply the replacement to abbreviated scientific names\n",
    "df['Title'] = df['Title'].apply(lambda x: surround_with_em(x, abbreviated_names))\n",
    "df['Abstract'] = df['Abstract'].apply(lambda x: surround_with_em(x, abbreviated_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf72e7e",
   "metadata": {},
   "source": [
    "### Re-formating References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6bb2d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns\n",
    "new_column_order = [\"Duplicate\", \"Type\", \"Author\", \"Year\", \"Title\", \"Journal Name\", \"Volume\", \"Issue\", \"Pages\", \"URL\", \"Location\", \"Specimen Data Entered\", \"Impacts Data Entered\", \"Keywords\", \"Comments\", \"Abstract\", \"DOI\", \"PDF Name\"]\n",
    "df = df[new_column_order]\n",
    "\n",
    "def highlight(x):\n",
    "    # Define columns to highlight for missing values and non-PDF file names\n",
    "    columns_to_highlight_na = [\"Type\", \"Author\", \"Year\", \"Title\", \"Journal Name\", \"Volume\", \"Pages\", \"Location\", \"Specimen Data Entered\", \"Impacts Data Entered\", \"PDF Name\"]\n",
    "    column_to_check_pdf = \"PDF Name\"\n",
    "    \n",
    "    # Create an empty DataFrame with the same index and columns as the input DataFrame\n",
    "    styled_df = pd.DataFrame('', index=x.index, columns=x.columns)\n",
    "    \n",
    "    # Highlight missing values\n",
    "    for col in columns_to_highlight_na:\n",
    "        mask_na = pd.isna(x[col])\n",
    "        styled_df.loc[mask_na, col] = f'background-color: #FFFF00;'\n",
    "    \n",
    "    # Handle NaN values in \"PDF Name\" column before applying bitwise NOT\n",
    "    mask_pdf_valid = ~x[column_to_check_pdf].astype(str).str.endswith('.pdf', na=False)\n",
    "    styled_df.loc[mask_pdf_valid, column_to_check_pdf] = 'background-color: #FFFF00;'\n",
    "    \n",
    "    # Return the styled DataFrame\n",
    "    return styled_df\n",
    "\n",
    "# Apply styling to the original DataFrame\n",
    "styled_df = df.style.apply(highlight, axis=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e02a0c9",
   "metadata": {},
   "source": [
    "## 5. Export file\n",
    "The following cell will format reference dataframe and export as an Excel sheet. A Word document with an error messages will also be saved in same directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50c42737",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DO NOT EDIT\n",
    "\n",
    "# Export Excel\n",
    "# Save Excel file\n",
    "root = Tk()                                         \n",
    "root.attributes(\"-topmost\", True)                   \n",
    "root.withdraw()\n",
    "file_path = filedialog.asksaveasfilename(defaultextension=\".xlsx\", filetypes=[(\"Excel files\", \"*.xlsx\"), (\"All files\", \"*.*\")])\n",
    "styled_df.to_excel(file_path, index=False)\n",
    "\n",
    "# Save Error Document\n",
    "directory = os.path.dirname(file_path)\n",
    "doc.save(os.path.join(directory, \"Keywords_Error_Doc.docx\")) \n",
    "\n",
    "# Open the Excel file\n",
    "# os.system(file_path)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d54bc5a",
   "metadata": {},
   "source": [
    "## 6. Final Edits:\n",
    "There is still some work that needs to be done before bulk uploading these references. The following is a checklist to follow\n",
    "\n",
    "1. There are certain column that can not be blank. Cells that can not be blank are also highlighted in yellow. Fix those cells.\n",
    "2. Check the format of reference columns in case EndNote missed them, particularly Type, Year, Journal Name, Pages, PDF Name. For example, make sure the Year column only has years.\n",
    "3. Check Reference Numbers in the Duplicate column to ensure they are a duplicate. Remove duplicate rows from the sheet.\n",
    "4. Double-check proper capitalization in titles - as stated above the model is not perfect.\n",
    "5. Add hypertext (&lt;em&gt;) around any extra scientific names in titles.\n",
    "6. Add subtext (&lt;sub&gt;) and supertext (&lt;sup&gt;) hypertext around numbers part of chemical formulas or units. This can also be done in NAs reference editor.\n",
    "7. Check Issue columns: its not a required field but sometimes EndNote puts in strange formats that need corrected.\n",
    "8. Check any remaining URLs: NAS requires website URL to link directly to website of journal  with PDF/html version of journal article if available (directly to website of journal). NAS does not want URLs that lead to indexing services (ProQuest, Web of Knowledge, EBSCOHost). \n",
    "9. Double check blank DOI cells: On rare occassions PDF has DOI but EndNote does report it.\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvRefEditor",
   "language": "python",
   "name": "venvrefeditor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
