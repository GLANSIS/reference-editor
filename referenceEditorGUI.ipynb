{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c18ecca1",
   "metadata": {},
   "source": [
    "# GLANSIS REFERENCE CLEANER & BULK UPLOADER\n",
    "**Description:** The following scripts will help you to better clean and bulk upload references\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acc860f",
   "metadata": {},
   "source": [
    "# PART 1: Reference Cleaner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24d1dad",
   "metadata": {},
   "source": [
    "## 1. Exporting from EndNote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bf9c18",
   "metadata": {},
   "source": [
    "To correctly get your Journal Article references out of Endnote into a text file that Jupyter Notebooks can read, see the following steps:\n",
    "1. Download and save the GLANSIS_refManagerExport.\n",
    "2. Select and open output style. It will open an EndNote output style editor.\n",
    "3. Click File > Save As.\n",
    "4. Close output style editor.\n",
    "5. Go to hte task bar and select the 'Biolographic Output Style' dropdown bar. Click 'Select Another Style.' Scroll to and select 'GLANSIS_refManagerExport.' Click 'Choose.'\n",
    "6. Select all the journal articles you want to enter into NAS.\n",
    "7. Click File > Export. In the pop-up menu, make to change preferences to:<br>\n",
    "    a. File name: Enter your file name of choice<br>\n",
    "    b. Save as type: Text File (.txt)<br>\n",
    "    c. Output style: GLANSIS_refManagerExport<br>\n",
    "    d. Click Save<br>\n",
    "8. To make it easier for the code to find a PDF, extract all PDFs from EndNote library and put into a new folder.<br>\n",
    "    a. Go to directory where the EndNote Library is stored. Click on the .Data folder.<br>\n",
    "    b. There will be a PDF and sdb folder inside. Click on the PDF folder.<br>\n",
    "    c. In the search bar type '.pdf'. Click on the first PDF and hit Ctrl + A.<br>\n",
    "    d. Copy and paste PDFs into a new folder. I recommend in the same folder as your EndNote Library outside your .Data file<br>\n",
    "9. Once this is done, you should be ready to start editing your\n",
    "    \n",
    "\n",
    "*This will only work for journal articles which is the bulk of what we handle. Any reports, websites, or other references will need to be entered by hand. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd117aaf",
   "metadata": {},
   "source": [
    "## 2. Run Widget to Clean References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e36b25d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\redinger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT\n",
    "\n",
    "import os                            # Mananages directories\n",
    "import numpy as np                   # Used to manage NA values\n",
    "import pandas as pd                  # Manages dataframes\n",
    "import re                            # Edits text strings\n",
    "import tkinter as tk                 # Constructs GUI\n",
    "from tkinter import Tk, filedialog   # Creates file dialog box\n",
    "import requests                      # Pulls HTML code from webpage\n",
    "from bs4 import BeautifulSoup        # HTML parsing\n",
    "import fitz                          # Open and pdf manipulation - package for PyMuPDF\n",
    "from docx import Document            # Create and edit Word Document\n",
    "from collections import OrderedDict  # Use to remove duplicates from keywords\n",
    "import unicodedata                   # Convert extracted keywords to unicode - removes issues with duplication\n",
    "import nltk                          # Library of natural language processing tools              \n",
    "import nltk.corpus                   # Access corpora\n",
    "import string                        # Format text strings\n",
    "import pickle                        # Serailizes and deserializing models\n",
    "import math                          # Math functions for numerical computations\n",
    "import nltk.data                     # Retrieves data files and resources\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer   # Concatenates token sequences\n",
    "from nltk.tokenize import word_tokenize   # Splits words into tokens\n",
    "nltk.download('punkt')                    # Pre-trained tokenizer\n",
    "from openpyxl import load_workbook        # Edit Excel sheets\n",
    "from openpyxl.styles import PatternFill   # Modify Excel formatting\n",
    "\n",
    "# Command to save species names\n",
    "def save_species_names():\n",
    "    \n",
    "    global species_names\n",
    "    \n",
    "    # Save species names entries\n",
    "    species_names = species_names_entry.get().split(',')\n",
    "    \n",
    "    # Check if species_names exists\n",
    "    if not species_names:\n",
    "        \n",
    "        # If species_id is empty, show an error message\n",
    "        save_success_label.config(text = \"Error - Species names required.\", fg=\"red\")\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # If species_id exists, proceed with saving\n",
    "        save_success_label.config(text = \"Success!\", fg=\"green\")\n",
    "\n",
    "# Command to save references\n",
    "def open_reference_file():\n",
    "\n",
    "    global df\n",
    "    \n",
    "    # Open a file dialog to select an Excel file\n",
    "    file_path = filedialog.askopenfilename(title=\"Select a File\", filetypes=[(\"Text Document\", \"*.txt\")])\n",
    "\n",
    "    # Create new column names because text file has no header\n",
    "    col_names = [\"Type\", \"Author\", \"Year\", \"Title\", \"Journal Name\", \"Volume\", \"Issue\", \"Pages\", \"URL\", \"Keywords\", \"Abstract\", \"DOI\", \"PDF Name\"]\n",
    "\n",
    "    # Convert text file into a dataframe\n",
    "    df = pd.read_csv(file_path, sep = '\\t', header = None, dtype = str, names = col_names, quotechar = '\"')\n",
    "    \n",
    "     # Check if df exists\n",
    "    if df.empty:\n",
    "        \n",
    "        # If df exists, proceed with saving\n",
    "        df_success_label.config(text = \"Error: Select .txt file with references.\", fg=\"red\")\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # If df is empty, show an error message\n",
    "        df_success_label.config(text = \"Success!\", fg=\"green\")\n",
    "\n",
    "# Command to save PDF folder\n",
    "def open_pdf_folder():\n",
    "    \n",
    "    global pdf_folder\n",
    "\n",
    "    # Open a file dialog to select PDF file location\n",
    "    pdf_folder = filedialog.askdirectory(title=\"Select a Folder\")\n",
    "    \n",
    "    # Check if pdf_folder exists\n",
    "    if not pdf_folder:\n",
    "        \n",
    "        # If df exists, proceed with saving\n",
    "        pdf_folder_success_label.config(text = \"Error: Select PDF folder.\", fg=\"red\")\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # If df is empty, show an error message\n",
    "        pdf_folder_success_label.config(text = \"Success!\", fg=\"green\")\n",
    "\n",
    "# Command to clean references\n",
    "def clean_references():\n",
    "    \n",
    "    global styled_df\n",
    "    global df\n",
    "    global doc\n",
    "    global species_names\n",
    "    \n",
    "    # Duplications \n",
    "\n",
    "    #url creation\n",
    "    url = 'https://nas.er.usgs.gov/queries/references/ReferenceList.aspx'\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "\n",
    "        # extract search parameters references\n",
    "        lead_author = str(row['Author']).split(',', 1)[0] + ','\n",
    "        date = row['Year']\n",
    "        title_start = ' '.join(str(row['Title']).split()[:5])\n",
    "\n",
    "        #set queary parameters\n",
    "        search_params = {\n",
    "            \"refnum\": '',\n",
    "            \"author\": lead_author,\n",
    "            \"date\": date,\n",
    "            \"title\":title_start,\n",
    "            \"journal\": '',\n",
    "            \"publisher\": '',\n",
    "            \"vol\": '',\n",
    "            \"issue\":'',\n",
    "            \"pages\":'',\n",
    "            \"URL\":'',\n",
    "            \"key_words\":'',\n",
    "            \"type\":'' \n",
    "        }\n",
    "\n",
    "        # call url\n",
    "        response = requests.post(url, params = search_params)\n",
    "\n",
    "        # scrape the RefNum \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "            #find specific tag using id attribute\n",
    "            desired_a_tag = soup.find(\"a\", {\"id\": \"ContentPlaceHolder1_GridView1_HyperLink1_0\"})\n",
    "\n",
    "            if desired_a_tag:\n",
    "                refnum = desired_a_tag.get_text(strip = True)\n",
    "                results.append(refnum)\n",
    "            else:\n",
    "                refnum = 'No'\n",
    "                results.append(refnum)\n",
    "\n",
    "     # add column\n",
    "    df['Duplicate'] = results \n",
    "\n",
    "\n",
    "    # DO NOT EDIT\n",
    "\n",
    "    # Add for location, specimen data, and impact data\n",
    "    df['Location'] = 'NAS'\n",
    "    df['Specimen Data Entered'] = 'N'\n",
    "    df['Impacts Data Entered'] = 'N'\n",
    "\n",
    "\n",
    "    # DO NOT EDIT\n",
    "\n",
    "    # Clean DOI\n",
    "    def clean_doi(text):\n",
    "        if pd.notna(text):\n",
    "            if 'doi.org' in text:\n",
    "                return text.replace('https://doi.org/', '')\n",
    "            else:\n",
    "                return text\n",
    "        else:\n",
    "            return text\n",
    "\n",
    "    df['DOI'] = df['DOI'].apply(clean_doi)\n",
    "\n",
    "\n",
    "    # Define a function to remove illegal characters\n",
    "    def remove_illegal_chars(text):\n",
    "        if pd.isnull(text):  # Check if the cell is empty\n",
    "            return ''\n",
    "        # Define the pattern for illegal characters\n",
    "        illegal_chars_pattern = re.compile(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F-\\x9F]')\n",
    "\n",
    "        # Replace illegal characters with an empty string\n",
    "        cleaned_text = illegal_chars_pattern.sub('', text)\n",
    "\n",
    "        return cleaned_text\n",
    "\n",
    "    df['Abstract'] = df['Abstract'].apply(remove_illegal_chars) \n",
    "\n",
    "    # URL Cleaning\n",
    "    def clear_cell(row):\n",
    "        if not pd.isna(row['DOI']):\n",
    "            return np.nan\n",
    "        else:\n",
    "            return row['URL']\n",
    "\n",
    "    df['URL'] = df.apply(clear_cell, axis = 1)\n",
    "\n",
    "    # Remove bad URLs\n",
    "    def remove_non_url(text):\n",
    "        if pd.notna(text):\n",
    "            if '<Go to ISI>:' in text:\n",
    "                return None\n",
    "            else:\n",
    "                return text\n",
    "\n",
    "    df['URL'] = df['URL'].apply(remove_non_url)\n",
    "\n",
    "    # Clean PDF file name\n",
    "    def clean_pdf_name(text):\n",
    "        if isinstance(text, str):\n",
    "            pdf_name = re.search(r'[^/]+$', text).group()\n",
    "            return pdf_name\n",
    "        else:\n",
    "            return text\n",
    "\n",
    "\n",
    "    df['PDF Name'] = df['PDF Name'].apply(clean_pdf_name)\n",
    "\n",
    "\n",
    "    # DO NOT EDIT\n",
    "    \n",
    "    # Create new word document for errors\n",
    "    doc = Document()\n",
    "    style = doc.styles['Normal']\n",
    "    style.paragraph_format.space_after = 1\n",
    "\n",
    "    # Add keywords\n",
    "    def keyword_find(filename):\n",
    "\n",
    "        # Open file\n",
    "        file = fitz.open(filename)\n",
    "\n",
    "        # Read and block only the first two pages of PDF\n",
    "        text = []\n",
    "        for i, page in enumerate(file):\n",
    "            if i > 1:\n",
    "                break\n",
    "            text += page.get_text(\"blocks\")\n",
    "\n",
    "        # Close file\n",
    "        file.close()\n",
    "\n",
    "        # Create if loop to account PDFs that can't be read in\n",
    "        if text:\n",
    "\n",
    "            # Find block containing keywords - if no keywords in PDF, common & scientific names used\n",
    "            for block in text:\n",
    "                if block[4].lower().startswith('key-words:'):\n",
    "                    pdf_keywords = block[4][10:].strip()\n",
    "                    break\n",
    "                elif block[4].lower().startswith('key words:'):\n",
    "                    pdf_keywords = block[4][10:].strip()\n",
    "                    break\n",
    "                elif block[4].lower().startswith('keywords:'):\n",
    "                    pdf_keywords = block[4][9:].strip()\n",
    "                    break\n",
    "                elif block[4].lower().startswith('key-words'):\n",
    "                    pdf_keywords = block[4][9:].strip()\n",
    "                    break\n",
    "                elif block[4].lower().startswith('key words'):\n",
    "                    pdf_keywords = block[4][9:].strip()\n",
    "                    break\n",
    "                elif block[4].lower().startswith('keywords'):\n",
    "                    pdf_keywords = block[4][8:].strip()\n",
    "                    break\n",
    "                else:\n",
    "                    pdf_keywords = '' \n",
    "\n",
    "        else:\n",
    "            pdf_keywords = '' \n",
    "\n",
    "        # Replace intermediate characters - this list is not exhaustive\n",
    "        keywords_replace = re.sub(r'[\\n;�\\xa0·./]', ', ', pdf_keywords).replace(', ,', ',').strip(',')\n",
    "\n",
    "        # Combine keywords with scientific and common names\n",
    "        clean_keywords = keywords_replace.split(',')\n",
    "\n",
    "        return(clean_keywords)\n",
    "        \n",
    "    # Remove breaks created by EndNote\n",
    "    df['Keywords'] = df['Keywords'].str.replace('\\r\\n', ', ')\n",
    "\n",
    "    # Convert each row into a list\n",
    "    df['Keywords'] = df['Keywords'].apply(lambda x: [x] if pd.notnull(x) else [])\n",
    "\n",
    "    # Create empty list for error messages\n",
    "    error_messages = []\n",
    "\n",
    "    # Iterate through DataFrame rows\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            # Create file path\n",
    "            file_path = os.path.join(pdf_folder, row['PDF Name'])\n",
    "\n",
    "            # Combine keywords entered by user, from EndNote, and from PDF - need to be lists not strings\n",
    "            combined_keywords = species_names + row['Keywords'] + keyword_find(file_path)\n",
    "\n",
    "            # Remove duplicate keywords\n",
    "            unique_keywords = list(OrderedDict.fromkeys(combined_keywords))\n",
    "\n",
    "            # Remove extra spaces around words in list\n",
    "            unique_keywords_strip = [word.strip() for word in unique_keywords] \n",
    "\n",
    "            # Combine list into text string\n",
    "            keywords = ', '.join(unique_keywords_strip)\n",
    "\n",
    "            # Update 'Keyword' column\n",
    "            df.at[index, 'Keywords'] = keywords\n",
    "\n",
    "        except Exception as e:\n",
    "            \n",
    "            # Create error message\n",
    "            error_text = f\"Error row {index + 1}: {e} \\nAuthors: {row['Author']} \\nYear: {row['Year']} \\nTitle: {row['Title']} \\nPDF: {row['PDF Name']}\\n\"\n",
    "            \n",
    "            # Append to error message list\n",
    "            error_messages.append(str(error_text))\n",
    "            \n",
    "            # Write error message to the Word document\n",
    "            doc.add_paragraph(f\"Error row {index + 1}: {e}\")\n",
    "            doc.add_paragraph(f\"Authors: {row['Author']}\")\n",
    "            doc.add_paragraph(f\"Year: {row['Year']}\")\n",
    "            doc.add_paragraph(f\"Title: {row['Title']}\")\n",
    "            doc.add_paragraph(\"\")  # Add an empty line between errors\n",
    "            \n",
    "            # Skip to the next iteration of the loop\n",
    "            continue\n",
    "\n",
    "    # Remove illegal characters\n",
    "    df['Keywords'] = df['Keywords'].apply(remove_illegal_chars) \n",
    "\n",
    "\n",
    "    # Create Truecaser to edit titles\n",
    "    class TrueCaser(object):\n",
    "        def __init__(self, dist_file_path=None):\n",
    "\n",
    "            \"\"\" Initialize module with the model from Google Drive \"\"\"\n",
    "            if dist_file_path is None:\n",
    "                dist_file_path = 'models/truecaserTest.dist'\n",
    "\n",
    "            with open(dist_file_path, \"rb\") as distributions_file:\n",
    "                pickle_dict = pickle.load(distributions_file)\n",
    "                self.uni_dist = pickle_dict[\"uni_dist\"]\n",
    "                self.backward_bi_dist = pickle_dict[\"backward_bi_dist\"]\n",
    "                self.forward_bi_dist = pickle_dict[\"forward_bi_dist\"]\n",
    "                self.trigram_dist = pickle_dict[\"trigram_dist\"]\n",
    "                self.word_casing_lookup = pickle_dict[\"word_casing_lookup\"]\n",
    "            self.detknzr = TreebankWordDetokenizer()\n",
    "\n",
    "        def get_score(self, prev_token, possible_token, next_token):\n",
    "            pseudo_count = 5.0\n",
    "\n",
    "            # Get Unigram Score\n",
    "            numerator = self.uni_dist[possible_token] + pseudo_count\n",
    "            denominator = 0\n",
    "            for alternativeToken in self.word_casing_lookup[\n",
    "                    possible_token.lower()]:\n",
    "                denominator += self.uni_dist[alternativeToken] + pseudo_count\n",
    "\n",
    "            unigram_score = numerator / denominator\n",
    "\n",
    "            # Get Backward Score\n",
    "            bigram_backward_score = 1\n",
    "            if prev_token is not None:\n",
    "                numerator = (\n",
    "                    self.backward_bi_dist[prev_token + \"_\" + possible_token] +\n",
    "                    pseudo_count)\n",
    "                denominator = 0\n",
    "                for alternativeToken in self.word_casing_lookup[\n",
    "                        possible_token.lower()]:\n",
    "                    denominator += (self.backward_bi_dist[prev_token + \"_\" +\n",
    "                                                          alternativeToken] +\n",
    "                                    pseudo_count)\n",
    "\n",
    "                bigram_backward_score = numerator / denominator\n",
    "\n",
    "            # Get Forward Score\n",
    "            bigram_forward_score = 1\n",
    "            if next_token is not None:\n",
    "                next_token = next_token.lower()  # Ensure it is lower case\n",
    "                numerator = (\n",
    "                    self.forward_bi_dist[possible_token + \"_\" + next_token] +\n",
    "                    pseudo_count)\n",
    "                denominator = 0\n",
    "                for alternativeToken in self.word_casing_lookup[\n",
    "                        possible_token.lower()]:\n",
    "                    denominator += (\n",
    "                        self.forward_bi_dist[alternativeToken + \"_\" + next_token] +\n",
    "                        pseudo_count)\n",
    "\n",
    "                bigram_forward_score = numerator / denominator\n",
    "\n",
    "            # Get Trigram Score\n",
    "            trigram_score = 1\n",
    "            if prev_token is not None and next_token is not None:\n",
    "                next_token = next_token.lower()  # Ensure it is lower case\n",
    "                numerator = (self.trigram_dist[prev_token + \"_\" + possible_token +\n",
    "                                               \"_\" + next_token] + pseudo_count)\n",
    "                denominator = 0\n",
    "                for alternativeToken in self.word_casing_lookup[\n",
    "                        possible_token.lower()]:\n",
    "                    denominator += (\n",
    "                        self.trigram_dist[prev_token + \"_\" + alternativeToken +\n",
    "                                          \"_\" + next_token] + pseudo_count)\n",
    "\n",
    "                trigram_score = numerator / denominator\n",
    "\n",
    "            result = (math.log(unigram_score) + math.log(bigram_backward_score) +\n",
    "                      math.log(bigram_forward_score) + math.log(trigram_score))\n",
    "\n",
    "            return result\n",
    "\n",
    "        def first_token_case(self, raw):\n",
    "            return raw.capitalize()\n",
    "\n",
    "        def get_true_case(self, sentence, out_of_vocabulary_token_option=\"title\"):\n",
    "            \"\"\" Wrapper function for handling untokenized input.\n",
    "\n",
    "            @param sentence: a sentence string to be tokenized\n",
    "            @param outOfVocabularyTokenOption:\n",
    "                title: Returns out of vocabulary (OOV) tokens in 'title' format\n",
    "                lower: Returns OOV tokens in lower case\n",
    "                as-is: Returns OOV tokens as is\n",
    "\n",
    "            Returns (str): detokenized, truecased version of input sentence\n",
    "            \"\"\"\n",
    "            tokens = word_tokenize(sentence)\n",
    "            tokens_true_case = self.get_true_case_from_tokens(tokens, out_of_vocabulary_token_option)\n",
    "            return self.detknzr.detokenize(tokens_true_case)\n",
    "\n",
    "        def get_true_case_from_tokens(self, tokens, out_of_vocabulary_token_option=\"title\"):\n",
    "            \"\"\" Returns the true case for the passed tokens.\n",
    "\n",
    "            @param tokens: List of tokens in a single sentence\n",
    "            @param pretokenised: set to true if input is alreay tokenised (e.g. string with whitespace between tokens)\n",
    "            @param outOfVocabularyTokenOption:\n",
    "                title: Returns out of vocabulary (OOV) tokens in 'title' format\n",
    "                lower: Returns OOV tokens in lower case\n",
    "                as-is: Returns OOV tokens as is\n",
    "\n",
    "            Returns (list[str]): truecased version of input list\n",
    "            of tokens\n",
    "            \"\"\"\n",
    "            tokens_true_case = []\n",
    "            for token_idx, token in enumerate(tokens):\n",
    "\n",
    "                if token in string.punctuation or token.isdigit():\n",
    "                    tokens_true_case.append(token)\n",
    "                else:\n",
    "                    token = token.lower()\n",
    "                    if token in self.word_casing_lookup:\n",
    "                        if len(self.word_casing_lookup[token]) == 1:\n",
    "                            tokens_true_case.append(\n",
    "                                list(self.word_casing_lookup[token])[0])\n",
    "                        else:\n",
    "                            prev_token = (tokens_true_case[token_idx - 1]\n",
    "                                          if token_idx > 0 else None)\n",
    "                            next_token = (tokens[token_idx + 1]\n",
    "                                          if token_idx < len(tokens) - 1 else None)\n",
    "\n",
    "                            best_token = None\n",
    "                            highest_score = float(\"-inf\")\n",
    "\n",
    "                            for possible_token in self.word_casing_lookup[token]:\n",
    "                                score = self.get_score(prev_token, possible_token,\n",
    "                                                       next_token)\n",
    "\n",
    "                                if score > highest_score:\n",
    "                                    best_token = possible_token\n",
    "                                    highest_score = score\n",
    "\n",
    "                            tokens_true_case.append(best_token)\n",
    "\n",
    "                        if token_idx == 0:\n",
    "                            tokens_true_case[0] = self.first_token_case(\n",
    "                                tokens_true_case[0])\n",
    "\n",
    "                    else:  # Token out of vocabulary\n",
    "                        if out_of_vocabulary_token_option == \"title\":\n",
    "                            tokens_true_case.append(token.title())\n",
    "                        elif out_of_vocabulary_token_option == \"capitalize\":\n",
    "                            tokens_true_case.append(token.capitalize())\n",
    "                        elif out_of_vocabulary_token_option == \"lower\":\n",
    "                            tokens_true_case.append(token.lower())\n",
    "                        else:\n",
    "                            tokens_true_case.append(token)\n",
    "\n",
    "            return tokens_true_case\n",
    "\n",
    "    # Upload Truecaser model\n",
    "    caser = TrueCaser('models/truecaserTest.dist')\n",
    "\n",
    "    # Function to apply get_true_case to a sentence\n",
    "    def apply_true_case(sentence):\n",
    "        return caser.get_true_case(sentence, \"lower\")\n",
    "\n",
    "\n",
    "    # Apply the function to the 'sentences' column\n",
    "    df['Title'] = df['Title'].apply(apply_true_case)\n",
    "\n",
    "    # Function to capitalize the first letter of the first word\n",
    "    def capitalize_first_word(text):\n",
    "        return text[:1].capitalize() + text[1:]\n",
    "\n",
    "    # Apply the function to the specified column\n",
    "    df['Title'] = df['Title'].apply(capitalize_first_word)\n",
    "\n",
    "\n",
    "    # Function to remove <i> and </i> tags from the text\n",
    "    def remove_italic_tags(text):\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "        return soup.get_text()\n",
    "\n",
    "    # Apply the function to the 'text_column' and store the result in a new column\n",
    "    df['Title'] = df['Title'].apply(remove_italic_tags)\n",
    "\n",
    "    # Upload text file with species names with subspecies list\n",
    "    species_names_with_subspecies = pd.read_excel('textFiles/subspecies.xlsx', header = None)\n",
    "    species_names_with_subspecies = species_names_with_subspecies.iloc[:, 0].to_list()\n",
    "\n",
    "    # Upload text file with species names list\n",
    "    species_names = pd.read_excel('textFiles/scientific_names.xlsx', header = None)\n",
    "    species_names = species_names.iloc[:, 0].to_list()\n",
    "\n",
    "    # Upload text file with genus list\n",
    "    genus = pd.read_excel('textFiles/genus.xlsx', header = None)\n",
    "    genus = genus.iloc[:, 0].to_list()\n",
    "\n",
    "    # Forumala to surround text with <em>\n",
    "    def surround_with_em(text, words_to_surround):\n",
    "        if isinstance(text, str):\n",
    "            for word in words_to_surround:\n",
    "                pattern = r'(?<!<em>)\\b' + re.escape(word.strip()) + r'\\b(?!<\\/em>)'\n",
    "                replacement = r'<em>\\g<0></em>'\n",
    "                text = re.sub(pattern, replacement, text)\n",
    "        return text\n",
    "\n",
    "\n",
    "    # Apply the replacement to species names with subspecies\n",
    "    df['Title'] = df['Title'].apply(lambda x: surround_with_em(x, species_names_with_subspecies))\n",
    "    # df['Abstract'] = df['Abstract'].apply(lambda x: surround_with_em(x, species_names_with_subspecies))\n",
    "\n",
    "    # Apply the replacement to species names\n",
    "    df['Title'] = df['Title'].apply(lambda x: surround_with_em(x, species_names))\n",
    "    # df['Abstract'] = df['Abstract'].apply(lambda x: surround_with_em(x, species_names))\n",
    "\n",
    "    # Apply the replacement to genus\n",
    "    df['Title'] = df['Title'].apply(lambda x: surround_with_em(x, genus))\n",
    "    \n",
    "    # Variation of genus usage make more work in abstract\n",
    "    # df['Abstract'] = df['Abstract'].apply(lambda x: surround_with_em(x, genus))\n",
    "\n",
    "\n",
    "    # Formula to abbreviate scientific names\n",
    "    def abbreviate_scientific_name(scientific_names):\n",
    "\n",
    "        abbreviated_names = []\n",
    "\n",
    "        for name in scientific_names:\n",
    "            words = name.split()\n",
    "            genus = words[0][0] + \".\"\n",
    "            species = \" \".join(words[1:])\n",
    "            abbreviated_names.append(genus + \" \" + species)\n",
    "\n",
    "        return abbreviated_names\n",
    "\n",
    "    abbreviated_names = abbreviate_scientific_name(species_names)\n",
    "\n",
    "    # Apply the replacement to abbreviated scientific names\n",
    "    df['Title'] = df['Title'].apply(lambda x: surround_with_em(x, abbreviated_names))\n",
    "    # df['Abstract'] = df['Abstract'].apply(lambda x: surround_with_em(x, abbreviated_names))\n",
    "    \n",
    "    # Reorder columns\n",
    "    new_column_order = [\"Duplicate\", \"Type\", \"Author\", \"Year\", \"Title\", \"Journal Name\", \"Volume\", \"Issue\", \"Pages\", \"URL\", \"Location\", \"Specimen Data Entered\", \"Impacts Data Entered\", \"Keywords\", \"Abstract\", \"DOI\", \"PDF Name\"]\n",
    "    df = df[new_column_order]\n",
    "\n",
    "    def highlight(x):\n",
    "        # Define columns to highlight for missing values and non-PDF file names\n",
    "        columns_to_highlight_na = [\"Type\", \"Author\", \"Year\", \"Title\", \"Journal Name\", \"Volume\", \"Pages\", \"Location\", \"Specimen Data Entered\", \"Impacts Data Entered\", \"PDF Name\"]\n",
    "        column_to_check_pdf = \"PDF Name\"\n",
    "\n",
    "        # Create an empty DataFrame with the same index and columns as the input DataFrame\n",
    "        styled_df = pd.DataFrame('', index=x.index, columns=x.columns)\n",
    "\n",
    "        # Highlight missing values\n",
    "        for col in columns_to_highlight_na:\n",
    "            mask_na = pd.isna(x[col])\n",
    "            styled_df.loc[mask_na, col] = f'background-color: #FFFF00;'\n",
    "\n",
    "        # Handle NaN values in \"PDF Name\" column before applying bitwise NOT\n",
    "        mask_pdf_valid = ~x[column_to_check_pdf].astype(str).str.endswith('.pdf', na=False)\n",
    "        styled_df.loc[mask_pdf_valid, column_to_check_pdf] = 'background-color: #FFFF00;'\n",
    "\n",
    "        # Return the styled DataFrame\n",
    "        return styled_df\n",
    "\n",
    "    # Apply styling to the original DataFrame\n",
    "    styled_df = df.style.apply(highlight, axis=None)\n",
    "    \n",
    "    \n",
    "    # Check if styled_df exists\n",
    "    if not styled_df:\n",
    "        \n",
    "        # If df exists, proceed with saving\n",
    "        styled_df_success_label.config(text = \"Error: Cleaning not completed!\", fg=\"red\")\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # If df is empty, show an error message\n",
    "        styled_df_success_label.config(text = \"Completed!\", fg=\"green\")\n",
    "    \n",
    "    # Create message box\n",
    "    def show_error_messages(messages):\n",
    "        \n",
    "        # Create a new top-level window\n",
    "        error_window = tk.Toplevel()\n",
    "        error_window.title(\"Error in Keywords\")\n",
    "\n",
    "        # Add a scrollbar\n",
    "        scrollbar = tk.Scrollbar(error_window)\n",
    "        scrollbar.pack(side=tk.RIGHT, fill=tk.Y)\n",
    "\n",
    "        # Create a text widget to display the error messages\n",
    "        text = tk.Text(error_window, wrap=tk.WORD, yscrollcommand=scrollbar.set, font=(\"Arial\", 9))\n",
    "        text.pack(expand=True, fill=tk.BOTH)\n",
    "        scrollbar.config(command=text.yview)\n",
    "\n",
    "        # Insert error messages into the text widget\n",
    "        for msg in messages:\n",
    "            text.insert(tk.END, msg + \"\\n\")\n",
    "\n",
    "        # Disable editing in the text widget\n",
    "        text.config(state=tk.DISABLED)\n",
    "\n",
    "        # Set the dimensions of the window\n",
    "        error_window.geometry(\"400x400\")  # Change dimensions as needed\n",
    "\n",
    "        # Make the window modal (disables interaction with other windows)\n",
    "        error_window.transient()\n",
    "        error_window.grab_set()\n",
    "\n",
    "        # Run the window's event loop\n",
    "        error_window.mainloop()\n",
    "        \n",
    "    # Keyword error message     \n",
    "    if error_messages:\n",
    "            \n",
    "        # Run function for error messages\n",
    "        show_error_messages(error_messages)\n",
    "            \n",
    "\n",
    "# Command to export Excel with references\n",
    "def export_excel():\n",
    "\n",
    "    # Export Excel\n",
    "    try:\n",
    "        \n",
    "        # Select where Excel should be saved\n",
    "        file_path = filedialog.asksaveasfilename(defaultextension=\".xlsx\", filetypes=[(\"Excel files\", \"*.xlsx\"), (\"All files\", \"*.*\")])\n",
    "        \n",
    "        # Export Excel with references\n",
    "        styled_df.to_excel(file_path, engine='openpyxl', index=False)\n",
    "        \n",
    "    except pd.errors.ParserError:\n",
    "        \n",
    "        # If illegal characters, send message\n",
    "        export_error_label.config(text = \"Error: Illegal characters found in the data.\", fg=\"red\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        \n",
    "        # If illegal characters, send message\n",
    "        export_error_label.config(text = \"Error: {e}\", fg=\"red\")\n",
    "\n",
    "    # Save Error Document\n",
    "    directory = os.path.dirname(file_path)\n",
    "    doc.save(os.path.join(directory, \"Keywords_Error_Doc.docx\"))\n",
    "    \n",
    "    # Open the Excel file\n",
    "    os.system(file_path) \n",
    "    \n",
    "\n",
    "# Create a tkinter window\n",
    "window = tk.Tk()\n",
    "window.attributes(\"-topmost\", True)\n",
    "window.title(\"Reference Editor\")\n",
    "\n",
    "# Header paragraph\n",
    "header_text = \"Reference Editor: The following widget will edit references from Endote and create an Excel sheet that can be bulk uploaded.\"\n",
    "header_label = tk.Label(window, text = header_text, wraplength = 500, justify = \"left\")\n",
    "header_label.grid(row = 0, padx = 10, pady = (10, 20), sticky = 'w')\n",
    "\n",
    "\n",
    "# Species ID entry fields\n",
    "species_names_label = tk.Label(window, text = \"Scientific & common names:\")\n",
    "species_names_label.grid(row = 1, padx = 10, pady = 5, sticky = \"w\")\n",
    "\n",
    "species_names_entry = tk.Entry(window, width = 45)\n",
    "species_names_entry.grid(row = 1, padx = (180, 0), pady = 5, sticky = \"w\")\n",
    "\n",
    "# Save button\n",
    "save_button = tk.Button(window, text = \"Save\", command = save_species_names, width = 15, height = 1)\n",
    "save_button.grid(row = 3, padx = 10, pady = 10, sticky = \"w\")\n",
    "save_success_label = tk.Label(window, text = \"\")\n",
    "save_success_label.grid(row = 3, padx = 150, pady = 10, sticky = \"w\")\n",
    "\n",
    "\n",
    "# Description of buttons\n",
    "button_text = \"Click buttons below to import reference information from Endnote\"\n",
    "button_label = tk.Label(window, text = button_text,  wraplength = 500, justify = \"left\")\n",
    "button_label.grid(row = 4, padx = 10, pady = (10, 0), sticky = \"w\")\n",
    "\n",
    "# Import references\n",
    "ref_import_button = tk.Button(window, text = \"Select .txt File\", command = open_reference_file, width = 15, height = 1)\n",
    "ref_import_button.grid(row = 5, padx = 10, pady = 10, sticky = \"w\")\n",
    "df_success_label = tk.Label(window, text = \"\")\n",
    "df_success_label.grid(row = 5, padx = 150, pady = 10, sticky = \"w\")\n",
    "\n",
    "# Select PDF folder\n",
    "pdf_button = tk.Button(window, text = \"Select PDF Folder\", command = open_pdf_folder, width = 15, height = 1)\n",
    "pdf_button.grid(row = 6, padx = 10, pady = 10, sticky = \"w\")\n",
    "pdf_folder_success_label = tk.Label(window, text = \"\")\n",
    "pdf_folder_success_label.grid(row = 6, padx = 150, pady = 10, sticky = \"w\")\n",
    "\n",
    "\n",
    "# Description of button\n",
    "clean_button_text = \"Click buttons below to clean references - will take time to load\"\n",
    "clean_button_label = tk.Label(window, text = clean_button_text,  wraplength = 450, justify = \"left\")\n",
    "clean_button_label.grid(row = 7, padx = 10, pady = (10, 0), sticky = \"w\")\n",
    "\n",
    "# Clean reference button\n",
    "clean_ref_button = tk.Button(window, text = \"Clean References\", command = clean_references, width = 15, height = 1)\n",
    "clean_ref_button.grid(row = 8, padx = 10, pady = 10, sticky = \"w\")\n",
    "styled_df_success_label = tk.Label(window, text = \"\")\n",
    "styled_df_success_label.grid(row = 8, padx = 150, pady = 10, sticky = \"w\")\n",
    "\n",
    "# Warning Note\n",
    "warning_text = checklist_text = \"\"\"** If you get you get a pop-up window with error messages, there was at least one error when pulling keywords. Most likely, there was an issue with the PDF file name. \"\"\"\n",
    "warning_label = tk.Label(window, text = warning_text, wraplength = 500, justify = \"left\")\n",
    "warning_label.grid(row = 9, padx = 10, pady = (10, 20), sticky = 'w')\n",
    "\n",
    "# Description of button\n",
    "export_button_text = \"Click button below to export reference information from Endnote\"\n",
    "export_button_label = tk.Label(window, text = export_button_text,  wraplength = 450, justify = \"left\")\n",
    "export_button_label.grid(row = 10, padx = 10, pady = (10, 0), sticky = \"w\")\n",
    "\n",
    "# Excel export button\n",
    "excel_export_button = tk.Button(window, text = \"Export Excel\", command = export_excel, width = 15, height = 1)\n",
    "excel_export_button.grid(row = 11, padx = 10, pady = (10, 20), sticky = \"w\")\n",
    "export_error_label = tk.Label(window, text = \"\")\n",
    "export_error_label.grid(row = 11, padx = 150, pady = 10, sticky = \"w\")\n",
    "\n",
    "# Quality Assurance Note\n",
    "qa_text = checklist_text = \"\"\"** There is still some work that needs to be done before bulk uploading these references. The following is a checklist to follow:\n",
    "\n",
    "1. There are certain columns that cannot be blank. Cells that cannot be blank are highlighted in yellow. Fix those cells.\n",
    "2. Check the format of reference columns in case EndNote missed them, particularly Type, Year, Journal Name, Pages, PDF Name. For example, make sure the Year column only has years.\n",
    "3. Check Reference Numbers in the Duplicate column to ensure they are a duplicate. Remove duplicate rows from the sheet.\n",
    "4. Double-check proper capitalization in titles - as stated above the model is not perfect.\n",
    "5. Add hypertext (<em>) around any extra scientific names in titles.\n",
    "6. Add subtext (<sub>) and supertext (<sup>) hypertext around numbers part of chemical formulas or units. This can also be done in NAS reference editor.\n",
    "7. Check Issue columns: it's not a required field but sometimes EndNote puts in strange formats that need to be corrected.\n",
    "8. Check any remaining URLs: NAS requires website URL to link directly to website of the journal with PDF/html version of the journal article if available (directly to the website of the journal). NAS does not want URLs that lead to indexing services (ProQuest, Web of Knowledge, EBSCOHost).\n",
    "9. Double check blank DOI cells: On rare occasions PDF has DOI but EndNote does not report it.\"\"\"\n",
    "\n",
    "qa_label = tk.Label(window, text = qa_text, wraplength = 500, justify = \"left\")\n",
    "qa_label.grid(row = 12, padx = 10, pady = (10, 20), sticky = 'w')\n",
    "\n",
    "# Run the tkinter event loop\n",
    "window.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8781f430",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvRefEditor",
   "language": "python",
   "name": "venvrefeditor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
